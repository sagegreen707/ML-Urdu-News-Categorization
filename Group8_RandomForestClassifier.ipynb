{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Columns: ['id', 'title', 'link', 'content', 'gold_label', 'content_length', 'cleaned_content']\n",
      "Number of samples: 1742\n",
      "Encoding labels...\n",
      "Unique labels after encoding: ['business', 'entertainment', 'science-technology', 'sports', 'world']\n",
      "Applying stemming to the text data...\n",
      "Converting text data into numerical features using TfidfVectorizer...\n",
      "Feature matrix shape: (1742, 1000)\n",
      "Dataset prepared by combining features and labels.\n",
      "Training samples: 1114, Validation samples: 279, Testing samples: 349\n",
      "Training Random Forest with 20 trees...\n",
      "Training tree 1/20...\n",
      "Building a decision tree...\n",
      "Tree built successfully.\n",
      "Training tree 2/20...\n",
      "Building a decision tree...\n",
      "Tree built successfully.\n",
      "Training tree 3/20...\n",
      "Building a decision tree...\n",
      "Tree built successfully.\n",
      "Training tree 4/20...\n",
      "Building a decision tree...\n",
      "Tree built successfully.\n",
      "Training tree 5/20...\n",
      "Building a decision tree...\n",
      "Tree built successfully.\n",
      "Training tree 6/20...\n",
      "Building a decision tree...\n",
      "Tree built successfully.\n",
      "Training tree 7/20...\n",
      "Building a decision tree...\n",
      "Tree built successfully.\n",
      "Training tree 8/20...\n",
      "Building a decision tree...\n",
      "Tree built successfully.\n",
      "Training tree 9/20...\n",
      "Building a decision tree...\n",
      "Tree built successfully.\n",
      "Training tree 10/20...\n",
      "Building a decision tree...\n",
      "Tree built successfully.\n",
      "Training tree 11/20...\n",
      "Building a decision tree...\n",
      "Tree built successfully.\n",
      "Training tree 12/20...\n",
      "Building a decision tree...\n",
      "Tree built successfully.\n",
      "Training tree 13/20...\n",
      "Building a decision tree...\n",
      "Tree built successfully.\n",
      "Training tree 14/20...\n",
      "Building a decision tree...\n",
      "Tree built successfully.\n",
      "Training tree 15/20...\n",
      "Building a decision tree...\n",
      "Tree built successfully.\n",
      "Training tree 16/20...\n",
      "Building a decision tree...\n",
      "Tree built successfully.\n",
      "Training tree 17/20...\n",
      "Building a decision tree...\n",
      "Tree built successfully.\n",
      "Training tree 18/20...\n",
      "Building a decision tree...\n",
      "Tree built successfully.\n",
      "Training tree 19/20...\n",
      "Building a decision tree...\n",
      "Tree built successfully.\n",
      "Training tree 20/20...\n",
      "Building a decision tree...\n",
      "Tree built successfully.\n",
      "Random Forest training completed.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from math import log2\n",
    "\n",
    "# Download NLTK data for stemming\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load Data\n",
    "df1 = pd.read_csv(r'D:\\Fall Semester 2024\\CS 438\\Model 1\\scraping_with_EDA\\scraping.ipynb\\cleaned_combined_articles.csv')\n",
    "\n",
    "print(\"Data loaded successfully.\")\n",
    "print(f\"Columns: {df1.columns.tolist()}\")\n",
    "print(f\"Number of samples: {len(df1)}\")\n",
    "\n",
    "# Encode Labels\n",
    "print(\"Encoding labels...\")\n",
    "le = LabelEncoder()\n",
    "df1['gold_label'] = le.fit_transform(df1['gold_label'])\n",
    "print(f\"Unique labels after encoding: {list(le.classes_)}\")\n",
    "\n",
    "# Apply Stemming\n",
    "print(\"Applying stemming to the text data...\")\n",
    "stemmer = PorterStemmer()\n",
    "df1['cleaned_content'] = df1['cleaned_content'].apply(lambda x: ' '.join([stemmer.stem(word) for word in x.split()]))\n",
    "\n",
    "# Convert Text Data to Features\n",
    "print(\"Converting text data into numerical features using TfidfVectorizer...\")\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X = vectorizer.fit_transform(df1['cleaned_content']).toarray()\n",
    "y = df1['gold_label'].values\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "\n",
    "# Combine Features and Labels for Custom Implementation\n",
    "dataset = np.hstack((X, y.reshape(-1, 1)))\n",
    "print(\"Dataset prepared by combining features and labels.\")\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "train, test = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state=42)\n",
    "print(f\"Training samples: {len(train)}, Validation samples: {len(val)}, Testing samples: {len(test)}\")\n",
    "\n",
    "# Define Entropy Impurity\n",
    "def entropy_impurity(groups, classes):\n",
    "    n_instances = float(sum([len(group) for group in groups]))\n",
    "    entropy = 0.0\n",
    "    for group in groups:\n",
    "        size = len(group)\n",
    "        if size == 0:\n",
    "            continue\n",
    "        score = 0.0\n",
    "        group_labels = [row[-1] for row in group]\n",
    "        for class_val in classes:\n",
    "            p = group_labels.count(class_val) / size\n",
    "            if p > 0:\n",
    "                score -= p * log2(p)\n",
    "        entropy += (score * (size / n_instances))\n",
    "    return entropy\n",
    "\n",
    "# Split Dataset\n",
    "def split_dataset(index, value, dataset):\n",
    "    left, right = [], []\n",
    "    for row in dataset:\n",
    "        if row[index] < value:\n",
    "            left.append(row)\n",
    "        else:\n",
    "            right.append(row)\n",
    "    return left, right\n",
    "\n",
    "# Get Best Split Using Entropy\n",
    "def get_best_split(dataset, n_features):\n",
    "    class_values = list(set(row[-1] for row in dataset))\n",
    "    features = np.random.choice(range(len(dataset[0]) - 1), n_features, replace=False)\n",
    "    best_index, best_value, best_score, best_groups = 999, 999, float('inf'), None\n",
    "    for index in features:\n",
    "        for row in dataset:\n",
    "            groups = split_dataset(index, row[index], dataset)\n",
    "            entropy = entropy_impurity(groups, class_values)\n",
    "            if entropy < best_score:\n",
    "                best_index, best_value, best_score, best_groups = index, row[index], entropy, groups\n",
    "    return {'index': best_index, 'value': best_value, 'groups': best_groups}\n",
    "\n",
    "# Create Terminal Node\n",
    "def create_terminal_node(group):\n",
    "    outcomes = [row[-1] for row in group]\n",
    "    return max(set(outcomes), key=outcomes.count)\n",
    "\n",
    "# Recursive Splitting\n",
    "def split(node, max_depth, min_size, n_features, depth):\n",
    "    left, right = node['groups']\n",
    "    del(node['groups'])\n",
    "    if not left or not right:\n",
    "        node['left'] = node['right'] = create_terminal_node(left + right)\n",
    "        return\n",
    "    if depth >= max_depth:\n",
    "        node['left'], node['right'] = create_terminal_node(left), create_terminal_node(right)\n",
    "        return\n",
    "    if len(left) <= min_size:\n",
    "        node['left'] = create_terminal_node(left)\n",
    "    else:\n",
    "        node['left'] = get_best_split(left, n_features)\n",
    "        split(node['left'], max_depth, min_size, n_features, depth + 1)\n",
    "    if len(right) <= min_size:\n",
    "        node['right'] = create_terminal_node(right)\n",
    "    else:\n",
    "        node['right'] = get_best_split(right, n_features)\n",
    "        split(node['right'], max_depth, min_size, n_features, depth + 1)\n",
    "\n",
    "# Build Tree\n",
    "def build_tree(train, max_depth, min_size, n_features):\n",
    "    print(\"Building a decision tree...\")\n",
    "    root = get_best_split(train, n_features)\n",
    "    split(root, max_depth, min_size, n_features, 1)\n",
    "    print(\"Tree built successfully.\")\n",
    "    return root\n",
    "\n",
    "# Predict\n",
    "def predict(node, row):\n",
    "    if row[node['index']] < node['value']:\n",
    "        if isinstance(node['left'], dict):\n",
    "            return predict(node['left'], row)\n",
    "        else:\n",
    "            return node['left']\n",
    "    else:\n",
    "        if isinstance(node['right'], dict):\n",
    "            return predict(node['right'], row)\n",
    "        else:\n",
    "            return node['right']\n",
    "\n",
    "# Random Forest Implementation\n",
    "class RandomForest:\n",
    "    def __init__(self, n_trees, max_depth, min_size, n_features):\n",
    "        self.n_trees = n_trees\n",
    "        self.max_depth = max_depth\n",
    "        self.min_size = min_size\n",
    "        self.n_features = n_features\n",
    "        self.trees = []\n",
    "\n",
    "    def subsample(self, dataset, ratio):\n",
    "        n_sample = round(len(dataset) * ratio)\n",
    "        return [dataset[i] for i in np.random.choice(len(dataset), n_sample, replace=True)]\n",
    "\n",
    "    def fit(self, train):\n",
    "        print(f\"Training Random Forest with {self.n_trees} trees...\")\n",
    "        for i in range(self.n_trees):\n",
    "            print(f\"Training tree {i + 1}/{self.n_trees}...\")\n",
    "            sample = self.subsample(train, 1.0)\n",
    "            tree = build_tree(sample, self.max_depth, self.min_size, self.n_features)\n",
    "            self.trees.append(tree)\n",
    "        print(\"Random Forest training completed.\")\n",
    "\n",
    "    def predict(self, row):\n",
    "        predictions = [predict(tree, row) for tree in self.trees]\n",
    "        return max(set(predictions), key=predictions.count)\n",
    "\n",
    "    def predict_dataset(self, test):\n",
    "        print(\"Making predictions on the dataset...\")\n",
    "        return [self.predict(row) for row in test]\n",
    "\n",
    "# Hyperparameters\n",
    "n_trees = 20  # Number of trees in the forest\n",
    "max_depth = 30  # Maximum depth of a tree\n",
    "min_size = 1  # Minimum samples per leaf node\n",
    "n_features = int(np.sqrt(X.shape[1]))  # Number of features to consider at each split\n",
    "\n",
    "# Initialize and Train the Random Forest\n",
    "rf = RandomForest(n_trees=n_trees, max_depth=max_depth, min_size=min_size, n_features=n_features)\n",
    "rf.fit(train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating on Validation Data...\n",
      "Making predictions on the dataset...\n",
      "Validation Accuracy: 0.95\n",
      "Validation Precision: 0.94\n",
      "Validation Recall: 0.95\n",
      "Validation F1-Score: 0.94\n",
      "\n",
      "Validation Confusion Matrix:\n",
      "[[70  0  0  0  0]\n",
      " [ 1 71  0  0  1]\n",
      " [ 2  2  3  0  3]\n",
      " [ 1  0  0 70  0]\n",
      " [ 1  2  1  1 50]]\n",
      "\n",
      "Evaluating on Test Data...\n",
      "Making predictions on the dataset...\n",
      "Test Accuracy: 0.92\n",
      "Test Precision: 0.92\n",
      "Test Recall: 0.92\n",
      "Test F1-Score: 0.92\n",
      "\n",
      "Test Confusion Matrix:\n",
      "[[67  0  0  1  2]\n",
      " [ 0 83  0  0  2]\n",
      " [ 4  3  6  0  5]\n",
      " [ 2  2  0 76  1]\n",
      " [ 1  2  1  1 90]]\n",
      "\n",
      "Classification Report on Test Data:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          business       0.91      0.96      0.93        70\n",
      "     entertainment       0.92      0.98      0.95        85\n",
      "science-technology       0.86      0.33      0.48        18\n",
      "            sports       0.97      0.94      0.96        81\n",
      "             world       0.90      0.95      0.92        95\n",
      "\n",
      "          accuracy                           0.92       349\n",
      "         macro avg       0.91      0.83      0.85       349\n",
      "      weighted avg       0.92      0.92      0.92       349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# Validate the model\n",
    "print(\"\\nEvaluating on Validation Data...\")\n",
    "val_predictions = rf.predict_dataset(val)\n",
    "val_labels = val[:, -1]\n",
    "\n",
    "val_accuracy = np.mean(val_predictions == val_labels)\n",
    "val_precision = precision_score(val_labels, val_predictions, average='weighted', zero_division=0)\n",
    "val_recall = recall_score(val_labels, val_predictions, average='weighted', zero_division=0)\n",
    "val_f1 = f1_score(val_labels, val_predictions, average='weighted', zero_division=0)\n",
    "val_conf_matrix = confusion_matrix(val_labels, val_predictions)\n",
    "\n",
    "print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
    "print(f\"Validation Precision: {val_precision:.2f}\")\n",
    "print(f\"Validation Recall: {val_recall:.2f}\")\n",
    "print(f\"Validation F1-Score: {val_f1:.2f}\")\n",
    "print(\"\\nValidation Confusion Matrix:\")\n",
    "print(val_conf_matrix)\n",
    "\n",
    "# Test the model\n",
    "print(\"\\nEvaluating on Test Data...\")\n",
    "test_predictions = rf.predict_dataset(test)\n",
    "test_labels = test[:, -1]\n",
    "\n",
    "test_accuracy = np.mean(test_predictions == test_labels)\n",
    "test_precision = precision_score(test_labels, test_predictions, average='weighted', zero_division=0)\n",
    "test_recall = recall_score(test_labels, test_predictions, average='weighted', zero_division=0)\n",
    "test_f1 = f1_score(test_labels, test_predictions, average='weighted', zero_division=0)\n",
    "test_conf_matrix = confusion_matrix(test_labels, test_predictions)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "print(f\"Test Precision: {test_precision:.2f}\")\n",
    "print(f\"Test Recall: {test_recall:.2f}\")\n",
    "print(f\"Test F1-Score: {test_f1:.2f}\")\n",
    "print(\"\\nTest Confusion Matrix:\")\n",
    "print(test_conf_matrix)\n",
    "\n",
    "# Print Detailed Classification Report\n",
    "print(\"\\nClassification Report on Test Data:\")\n",
    "print(classification_report(test_labels, test_predictions, target_names=le.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
