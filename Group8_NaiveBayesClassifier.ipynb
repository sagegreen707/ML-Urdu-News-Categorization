{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df1 = pd.read_csv(r'D:\\Fall Semester 2024\\CS 438\\Model 1\\scraping_with_EDA\\scraping.ipynb\\cleaned_combined_articles.csv')\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "texts = df1['cleaned_content']\n",
    "labels = df1['gold_label']\n",
    "\n",
    "# tokenization\n",
    "def preprocess(text):\n",
    "    return text.split()\n",
    "\n",
    "processed_texts = texts.apply(preprocess)\n",
    "\n",
    "#add each word to the vocabulary. \n",
    "vocabulary = set()\n",
    "for text in processed_texts:\n",
    "    for word in text:\n",
    "        vocabulary.add(word)\n",
    "\n",
    "# vocab_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "vocab_to_index = {}\n",
    "index = 0\n",
    "for word in vocabulary:\n",
    "    vocab_to_index[word] = index\n",
    "    index += 1\n",
    "\n",
    "def encode_text(text):\n",
    "    vector = np.zeros(len(vocabulary))\n",
    "    for word in text:\n",
    "        if word in vocab_to_index:\n",
    "            vector[vocab_to_index[word]] += 1\n",
    "    return vector\n",
    "\n",
    "#storing the frequencies of each word in X array\n",
    "X = np.array([encode_text(text) for text in processed_texts])\n",
    "y = labels.values\n",
    "\n",
    "\n",
    "def train_test_split(X, y, test_size=0.2, random_state=30):\n",
    "    np.random.seed(random_state)\n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    split_i = int(len(X) * (1 - test_size))\n",
    "    train_indices= indices[:split_i]\n",
    "    test_indices = indices[split_i:]\n",
    "    return X[train_indices], X[test_indices], y[train_indices], y[test_indices]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "class NaiveBayesClassifier:\n",
    "    def __init__(self):\n",
    "        self.class_probs = {}\n",
    "        self.feature_probs = {}\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        \n",
    "        # count the occurrences of each class in y\n",
    "        class_counts = {}\n",
    "        for label in y:\n",
    "            if label in class_counts:\n",
    "                class_counts[label] += 1\n",
    "            else:\n",
    "                class_counts[label] = 1\n",
    "\n",
    "        #calculate class probabilities (P(class))\n",
    "        class_probs = {}\n",
    "        for c, count in class_counts.items():\n",
    "            prob = count / n_samples\n",
    "            class_probs[c] = prob\n",
    "        self.class_probs = class_probs\n",
    "\n",
    "        self.feature_probs = {c: np.zeros(n_features) for c in class_counts}\n",
    "        for c in class_counts:\n",
    "            X_c = X[y == c]  \n",
    "            feature_sums = np.sum(X_c, axis=0) + 1  \n",
    "            self.feature_probs[c] = feature_sums / feature_sums.sum()\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            class_scores = {}\n",
    "            for c in self.class_probs:\n",
    "                log_prob = np.log(self.class_probs[c])\n",
    "                log_prob += np.sum(np.log(self.feature_probs[c]) * x)\n",
    "                class_scores[c] = log_prob\n",
    "            predictions.append(max(class_scores, key=class_scores.get))\n",
    "        return np.array(predictions)\n",
    "\n",
    "nb_model = NaiveBayesClassifier()\n",
    "nb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb_model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9828080229226361\n",
      "Confusion Matrix:\n",
      "[[78  0  0  2  1]\n",
      " [ 0 94  1  0  0]\n",
      " [ 0  0 11  0  0]\n",
      " [ 0  0  0 86  0]\n",
      " [ 1  0  1  0 74]]\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "          business       0.99      0.96      0.97        81\n",
      "     entertainment       1.00      0.99      0.99        95\n",
      "science-technology       0.85      1.00      0.92        11\n",
      "            sports       0.98      1.00      0.99        86\n",
      "             world       0.99      0.97      0.98        76\n",
      "\n",
      "          accuracy                           0.98       349\n",
      "         macro avg       0.96      0.99      0.97       349\n",
      "      weighted avg       0.98      0.98      0.98       349\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def accuracy(y_true, y_pred):\n",
    "    correct_predictions = np.sum(y_true == y_pred)\n",
    "    return correct_predictions / len(y_true)\n",
    "\n",
    "print(\"Accuracy:\", accuracy(y_test, y_pred))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
